# build_v5.py
import time
import gc
import os
import logging
from tqdm import tqdm
from concurrent.futures import ProcessPoolExecutor, as_completed

from utils.doc_to_docx import convert_doc_to_docx
# Import c√°c h√†m c·∫ßn thi·∫øt
import config
from db.weaviateDB import connect_to_weaviate
from rag_components import (
    get_huggingface_embeddings,
    create_weaviate_schema_if_not_exists,
    ingest_chunks_with_native_batching,
    filter_and_serialize_complex_metadata
)
from utils.process_data import process_single_file_comprehensive


logger = logging.getLogger(__name__)

# --- C√ÅC H√ÄM H·ªñ TR·ª¢ CHO CHECKPOINTING ---
def load_processed_files() -> set:
    """ƒê·ªçc file checkpoint v√† tr·∫£ v·ªÅ m·ªôt set c√°c t√™n file ƒë√£ x·ª≠ l√Ω."""
    if not os.path.exists(config.CHECKPOINT_FILE):
        return set()
    with open(config.CHECKPOINT_FILE, 'r', encoding='utf-8') as f:
        return {line.strip() for line in f if line.strip()}

def log_processed_file(filename: str):
    """Ghi t√™n file ƒë√£ x·ª≠ l√Ω th√†nh c√¥ng v√†o file checkpoint."""
    with open(config.CHECKPOINT_FILE, 'a', encoding='utf-8') as f:
        f.write(filename + '\n')

def clear_weaviate_collection(client, collection_name: str):
    """H√†m helper ƒë·ªÉ x√≥a collection v√† file checkpoint."""
    try:
        if client.collections.exists(collection_name):
            logger.warning(f"üóëÔ∏è ƒêang x√≥a collection '{collection_name}'...")
            client.collections.delete(collection_name)
            logger.info(f"‚úÖ ƒê√£ x√≥a collection '{collection_name}' th√†nh c√¥ng.")

        # X√≥a c·∫£ file checkpoint khi rebuild
        if os.path.exists(config.CHECKPOINT_FILE):
            os.remove(config.CHECKPOINT_FILE)
            logger.info(f"üóëÔ∏è ƒê√£ x√≥a file checkpoint '{config.CHECKPOINT_FILE}'.")

    except Exception as e:
        logger.error(f"‚ùå L·ªói khi x√≥a collection: {e}")
        raise e

def build_store_v5(force_rebuild: bool = False, pool_batch_size: int = 50):
    """
    H√†m x√¢y d·ª±ng Vector Store v·ªõi Checkpointing v√† Pool Restart ƒë·ªÉ x·ª≠ l√Ω c√°c job d√†i h∆°i.

    Args:
        force_rebuild (bool): X√≥a d·ªØ li·ªáu c≈© v√† b·∫Øt ƒë·∫ßu l·∫°i t·ª´ ƒë·∫ßu.
        pool_batch_size (int): S·ªë l∆∞·ª£ng file x·ª≠ l√Ω tr∆∞·ªõc khi t√°i kh·ªüi ƒë·ªông worker pool.
    """
    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu Qu√° tr√¨nh X√¢y d·ª±ng Vector Store v5 (Checkpointing & Pool Restart) üöÄ")
    total_start_time = time.time()
    total_chunks_indexed = 0
    weaviate_client = None

    try:
        # --- 1. SETUP PHASE ---
        logger.info("‚öôÔ∏è  1. Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng...")
        device = 'cpu' # Gi·∫£ ƒë·ªãnh kh√¥ng c√≥ GPU
        logger.info(f"üíª S·ª≠ d·ª•ng thi·∫øt b·ªã: {device}")

        weaviate_client = connect_to_weaviate(run_diagnostics=False)
        if not weaviate_client: raise ConnectionError("Kh√¥ng th·ªÉ k·∫øt n·ªëi ƒë·∫øn Weaviate.")

        logger.info("üß† T·∫£i model embedding (ch·∫°y tr√™n CPU)...")
        embeddings_model = get_huggingface_embeddings(config.EMBEDDING_MODEL_NAME, device)
        if not embeddings_model: raise RuntimeError("Kh√¥ng th·ªÉ kh·ªüi t·∫°o model embedding.")

        collection_name = config.WEAVIATE_COLLECTION_NAME

        if force_rebuild:
            clear_weaviate_collection(weaviate_client, collection_name)

        create_weaviate_schema_if_not_exists(weaviate_client, collection_name)

        # --- 2. L·ªåC FILE V√Ä CHU·∫®N B·ªä BATCH ---
        processed_files = load_processed_files()
        all_files = os.listdir(config.CORE_DATA_FOLDER)
        all_txt_paths = [os.path.join(config.CORE_DATA_FOLDER, f) for f in all_files if f.lower().endswith('.txt')]
        all_docx_paths = [os.path.join(config.CORE_DATA_FOLDER, f) for f in all_files if f.lower().endswith('.docx')]
        all_doc_paths = [os.path.join(config.CORE_DATA_FOLDER, f) for f in all_files if f.lower().endswith('.doc')]

        # --- Chuy·ªÉn .doc sang .docx ---
        for doc_path in all_doc_paths:
            try:
                convert_doc_to_docx(doc_path, config.CORE_DATA_FOLDER)
                os.remove(doc_path)
                logger.info(f"‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi {doc_path} th√†nh .docx")
            except Exception as e:
                logger.error(f"‚ùå Kh√¥ng th·ªÉ chuy·ªÉn file DOC: {doc_path} ‚Üí {e}")

        # C·∫≠p nh·∫≠t l·∫°i danh s√°ch .docx sau khi chuy·ªÉn
        all_docx_paths = [os.path.join(config.CORE_DATA_FOLDER, f)
                          for f in os.listdir(config.CORE_DATA_FOLDER)
                          if f.lower().endswith('.docx')]

        # T·ªïng h·ª£p file c·∫ßn x·ª≠ l√Ω
        all_file_paths = all_txt_paths + all_docx_paths

        # L·ªçc ra nh·ªØng file ch∆∞a ƒë∆∞·ª£c x·ª≠ l√Ω
        files_to_process = [path for path in all_file_paths if os.path.basename(path) not in processed_files]

        if not files_to_process:
            logger.info("‚úÖ T·∫•t c·∫£ c√°c file ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω. Kh√¥ng c√≥ g√¨ ƒë·ªÉ l√†m.")
            return

        logger.info(f"üîç ƒê√£ x·ª≠ l√Ω {len(processed_files)} files. C√≤n l·∫°i {len(files_to_process)} files c·∫ßn x·ª≠ l√Ω.")

        # --- 3. X·ª¨ L√ù THEO L√î ƒê·ªÇ CH·ªêNG R√í R·ªà B·ªò NH·ªö ---
        #max_workers = os.cpu_count() or 1
        max_workers = min(2, os.cpu_count() or 1)# Gi·ªõi h·∫°n worker ƒë·ªÉ tr√°nh qu√° t·∫£i gemini

        # T·∫°o thanh ti·∫øn tr√¨nh t·ªïng
        main_progress_bar = tqdm(total=len(files_to_process), desc="T·ªïng ti·∫øn tr√¨nh")

        # Chia danh s√°ch file th√†nh c√°c l√¥ nh·ªè
        for i in range(0, len(files_to_process), pool_batch_size):
            file_batch = files_to_process[i:i + pool_batch_size]
            logger.info(f"\nüîÑ ƒêang x·ª≠ l√Ω l√¥ {i//pool_batch_size + 1}, g·ªìm {len(file_batch)} files. T√°i kh·ªüi ƒë·ªông Worker Pool...")

            # T·∫°o m·ªôt Worker Pool M·ªöI cho m·ªói l√¥
            with ProcessPoolExecutor(max_workers=max_workers) as executor:
                future_to_file = {executor.submit(process_single_file_comprehensive, path): path for path in file_batch}

                for future in as_completed(future_to_file):
                    path = future_to_file[future]
                    filename = os.path.basename(path)
                    try:
                        chunks_from_file = future.result()
                        if not chunks_from_file:
                            logger.warning(f"File '{filename}' kh√¥ng c√≥ chunks.")
                            log_processed_file(filename) # V·∫´n ghi nh·∫≠n l√† ƒë√£ x·ª≠ l√Ω
                            main_progress_bar.update(1)
                            continue

                        processed_chunks = filter_and_serialize_complex_metadata(chunks_from_file)

                        # L∆∞u metadata ra file JSON tr∆∞·ªõc khi ingest l√™n Weaviate
                        import json
                        metadata_list = [chunk.metadata for chunk in processed_chunks]
                        metadata_dir = os.path.join(os.path.dirname(path), "..", "processed_files_metadata")
                        os.makedirs(metadata_dir, exist_ok=True)
                        metadata_filename = os.path.splitext(os.path.basename(path))[0] + "_metadata.json"
                        metadata_path = os.path.join(metadata_dir, metadata_filename)
                        with open(metadata_path, "w", encoding="utf-8") as meta_f:
                            json.dump(metadata_list, meta_f, ensure_ascii=False, indent=2)

                        ingest_chunks_with_native_batching(
                            client=weaviate_client,
                            collection_name=collection_name,
                            chunks=processed_chunks,
                            embeddings_model=embeddings_model
                        )

                        total_chunks_indexed += len(chunks_from_file)
                        log_processed_file(filename) # Ghi nh·∫≠n th√†nh c√¥ng
                        main_progress_bar.set_description(f"‚úÖ Ingested '{filename}'")
                        main_progress_bar.update(1) # C·∫≠p nh·∫≠t thanh ti·∫øn tr√¨nh t·ªïng

                        del chunks_from_file, processed_chunks
                        gc.collect()

                    except Exception as e:
                        logger.error(f"‚ùå L·ªói khi x·ª≠ l√Ω file '{filename}': {e}", exc_info=True)

        main_progress_bar.close()
        logger.info(f"\n‚úÖ ƒê√£ ingest th√†nh c√¥ng th√™m {total_chunks_indexed} chunks.")

    except (ConnectionError, RuntimeError, Exception) as e:
        logger.error(f"üí• ƒê√£ x·∫£y ra l·ªói nghi√™m tr·ªçng: {e}", exc_info=True)

    finally:
        # --- 4. D·ªåN D·∫∏P ---
        logger.info("\n‚öôÔ∏è  4. D·ªçn d·∫πp t√†i nguy√™n...")
        if 'embeddings_model' in locals(): del embeddings_model
        if weaviate_client and weaviate_client.is_connected():
            weaviate_client.close()
            logger.info("üîå ƒê√£ ƒë√≥ng k·∫øt n·ªëi Weaviate.")
        gc.collect()
        total_end_time = time.time()
        logger.info(f"‚è±Ô∏è  T·ªïng th·ªùi gian ch·∫°y l·∫ßn n√†y: {total_end_time - total_start_time:.2f} gi√¢y")
        logger.info("üéâ Ch∆∞∆°ng tr√¨nh k·∫øt th√∫c! üéâ")

if __name__ == "__main__":
    # Khi ch·∫°y l·∫°i, ch·ªâ c·∫ßn ch·∫°y l·ªánh n√†y. N√≥ s·∫Ω t·ª± ƒë·ªông ti·∫øp t·ª•c.
    # ƒê·∫∑t force_rebuild=True ch·ªâ khi b·∫°n mu·ªën x√≥a s·∫°ch v√† l√†m l·∫°i t·ª´ ƒë·∫ßu.
    build_store_v5(force_rebuild=True)