from langchain_huggingface import HuggingFaceEmbeddings
import config
import prompt_templete
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableLambda, RunnablePassthrough
from langchain_core.documents import Document
import logging
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from typing import List,Any,Dict
from langchain_weaviate.vectorstores import WeaviateVectorStore
from langchain_google_genai import ChatGoogleGenerativeAI
from utils.process_data import filter_and_serialize_complex_metadata
import weaviate
import weaviate.classes.config as wvc_config
from weaviate.exceptions import WeaviateQueryException
import time
from operator import itemgetter


logger = logging.getLogger(__name__)

WEAVIATE_SCHEMA_CONFIG: List[Dict[str, Any]] = [
    # T√™n tr∆∞·ªùng, Ki·ªÉu d·ªØ li·ªáu trong Weaviate, C√≥ n√™n vector h√≥a tr∆∞·ªùng n√†y kh√¥ng?
    {"name": "source", "dataType": wvc_config.DataType.TEXT,"index_searchable": False, "vectorize": False},
    {"name": "title", "dataType": wvc_config.DataType.TEXT, "index_searchable": True, "tokenization": wvc_config.Tokenization.WORD, "vectorize": True},
    {"name": "field", "dataType": wvc_config.DataType.TEXT,"index_searchable": True, "vectorize": True},
    {"name": "so_hieu", "dataType": wvc_config.DataType.TEXT, "index_searchable": False,"vectorize": False},
    #{"name": "so_hieu", "dataType": wvc_config.DataType.TEXT, "index_searchable": True,"vectorize": True},
    {"name": "loai_van_ban", "dataType": wvc_config.DataType.TEXT, "index_searchable": True,"vectorize": True},
    {"name": "ten_van_ban", "dataType": wvc_config.DataType.TEXT,"index_searchable": True, "tokenization": wvc_config.Tokenization.WORD, "vectorize": True},
    {"name": "co_quan_ban_hanh", "dataType": wvc_config.DataType.TEXT, "index_searchable": False,"vectorize": False},
    {"name": "ngay_ban_hanh_str", "dataType": wvc_config.DataType.TEXT,"index_searchable": False, "vectorize": False},
    {"name": "nam_ban_hanh", "dataType": wvc_config.DataType.INT,"index_searchable": True, "vectorize": False},
    {"name": "phan_code", "dataType": wvc_config.DataType.TEXT,"index_searchable": False, "vectorize": False},
    {"name": "chuong_code", "dataType": wvc_config.DataType.TEXT, "index_searchable": False,"vectorize": False},
    {"name": "muc_code", "dataType": wvc_config.DataType.TEXT,"index_searchable": False, "vectorize": False},
    {"name": "dieu_code", "dataType": wvc_config.DataType.TEXT,"index_searchable": False, "vectorize": False},
    {"name": "entity_type", "dataType": wvc_config.DataType.TEXT,"index_searchable": True, "vectorize": False},
    {"name": "penalties", "dataType": wvc_config.DataType.TEXT,"index_searchable": False, "vectorize": False},
    {"name": "cross_references", "dataType": wvc_config.DataType.TEXT, "index_searchable": False, "vectorize": False},
]

# H√†m get_huggingface_embeddings gi·ªØ nguy√™n
def get_huggingface_embeddings(model_name: str, device: str = 'cpu'):
    logger.info(f"üî∏ƒêang kh·ªüi t·∫°o model embedding: {model_name} tr√™n thi·∫øt b·ªã {device}...")

    model_kwargs = {
        'device': device,
        'trust_remote_code': True  # th√™m ƒë·ªÉ ƒë·∫£m b·∫£o load ƒë∆∞·ª£c nh·ªØng model custom
    }
    encode_kwargs = {
        'batch_size': 32,  # k√≠ch th∆∞·ªõc batch cho embedding
        'normalize_embeddings': True  # normalize ƒë·ªÉ cosine similarity chu·∫©n
    }

    try:
        embeddings = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs=model_kwargs,
            encode_kwargs=encode_kwargs
        )
        logger.info("üî∏Kh·ªüi t·∫°o model embedding th√†nh c√¥ng.")
        return embeddings
    except Exception as e:
        logger.error(f"üî∏L·ªói khi kh·ªüi t·∫°o model embedding: {e}")
        raise Exception(f"Kh·ªüi t·∫°o model embedding th·∫•t b·∫°i: {str(e)}")

# Begin New

def create_weaviate_schema_if_not_exists(client: weaviate.WeaviateClient, collection_name: str):
    """
    C·∫¢I TI·∫æN: T·∫°o schema v·ªõi c·∫•u h√¨nh chi ti·∫øt cho filtering v√† hybrid search.
    """
    if client.collections.exists(collection_name):
        logger.info(f"‚úÖ Schema for collection '{collection_name}' already exists.")
        return

    logger.info(f"üî∏ Schema for collection '{collection_name}' not found. Creating...")
    try:
        properties = []
        for prop_config in WEAVIATE_SCHEMA_CONFIG:
            properties.append(
                wvc_config.Property(
                    name=prop_config["name"],
                    data_type=prop_config["dataType"],
                    # B·ªè qua vector h√≥a n·∫øu vectorize=False ho·∫∑c kh√¥ng ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a
                    skip_vectorization=not prop_config.get("vectorize", False),
                    # K√≠ch ho·∫°t tokenization cho c√°c tr∆∞·ªùng c·∫ßn t√¨m ki·∫øm t·ª´ kh√≥a
                    tokenization=prop_config.get("tokenization")
                )
            )

        # Th√™m tr∆∞·ªùng 'text' ch√≠nh, t·ªëi ∆∞u cho c·∫£ vector v√† keyword search
        properties.append(
            wvc_config.Property(
                name="text",
                data_type=wvc_config.DataType.TEXT,
                skip_vectorization=False, # Lu√¥n vector h√≥a n·ªôi dung ch√≠nh
                tokenization=wvc_config.Tokenization.WORD # Cho ph√©p t√¨m ki·∫øm BM25 tr√™n n·ªôi dung
            )
        )

        client.collections.create(
            name=collection_name,
            properties=properties,
            # K√≠ch ho·∫°t inverted index (b·∫Øt bu·ªôc cho filtering v√† BM25)
            inverted_index_config=wvc_config.Configure.inverted_index(
                index_null_state=True,
                index_property_length=True,
                index_timestamps=True,
                bm25_b=0.75,  # Tham s·ªë BM25, c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh
                bm25_k1=1.2   # Tham s·ªë K1 cho BM25
            ),

            vectorizer_config=wvc_config.Configure.Vectorizer.none(),
            vector_index_config=wvc_config.Configure.VectorIndex.hnsw(
                distance_metric=wvc_config.VectorDistances.COSINE
            )
        )
        logger.info(f"‚úÖ Successfully created schema for collection '{collection_name}'.")
    except WeaviateQueryException as e:
        logger.error(f"‚ùå Error creating schema: {e}", exc_info=True)
        raise

def ingest_chunks_with_native_batching(client: weaviate.WeaviateClient, collection_name: str, chunks: List[Document], embeddings_model):
    """S·ª≠ d·ª•ng API batch g·ªëc c·ªßa Weaviate, an to√†n v√† hi·ªáu su·∫•t cao."""
    logger.info(f"üöÄ B·∫Øt ƒë·∫ßu qu√° tr√¨nh ingestion cho {len(chunks)} chunks...")

    texts_to_embed = [chunk.page_content for chunk in chunks]

    logger.info(f"üß† ƒêang t·∫°o embeddings cho {len(texts_to_embed)} chunks...")
    start_embed_time = time.time()
    chunk_vectors = embeddings_model.embed_documents(texts_to_embed)
    logger.info(f"‚è±Ô∏è  Th·ªùi gian t·∫°o embedding: {time.time() - start_embed_time:.2f} gi√¢y.")

    # 3. C·∫¢I TI·∫æN: ƒê·∫£m b·∫£o ch·ªâ ingest c√°c thu·ªôc t√≠nh h·ª£p l·ªá
    valid_property_names = {prop["name"] for prop in    WEAVIATE_SCHEMA_CONFIG}
    valid_property_names.add("text") # Th√™m tr∆∞·ªùng 'text'

    with client.batch.dynamic() as batch:
        for i, chunk in enumerate(chunks):
            if not isinstance(chunk,Document) or not hasattr(chunk, 'id') or not chunk.id:
                logger.warning(f"B·ªè qua chunk ·ªü v·ªã tr√≠ {i} do kh√¥ng h·ª£p l·ªá (sai type ho·∫∑c thi·∫øu ID).")
                continue

            properties = {"text": chunk.page_content}
            # L·ªçc metadata ƒë·ªÉ ch·ªâ gi·ªØ l·∫°i c√°c key h·ª£p l·ªá ƒë√£ ƒë·ªãnh nghƒ©a trong schema
            filtered_metadata = {
                k: v for k, v in chunk.metadata.items() if k in valid_property_names
            }
            properties.update(filtered_metadata)

            batch.add_object(
                collection=collection_name,
                properties=properties,
                uuid=chunk.id,
                vector=chunk_vectors[i]
            )

    logger.info(f"‚úÖ Batching ho√†n t·∫•t. ƒê√£ g·ª≠i {len(chunks)} objects.")
    if batch.number_errors > 0:
        logger.error(f"‚ùå C√≥ {batch.number_errors} l·ªói x·∫£y ra trong qu√° tr√¨nh batching.")
        # Log ra 5 l·ªói ƒë·∫ßu ti√™n ƒë·ªÉ d·ªÖ g·ª° l·ªói
        for i, error_msg in enumerate(batch.errors):
            if i >= 5: break
            logger.error(f"  - L·ªói {i+1}: {error_msg}")


# End new

def create_or_load_vectorstore(embeddings, weaviate_url, collection_name, weaviate_client, chunks=None):
    vectorstore = None

    if not embeddings:
        logger.error("üî∏Kh√¥ng c√≥ model embedding ƒë·ªÉ t·∫°o/t·∫£i vector store.")
        return None

    logger.info(f"üî∏Truy c·∫≠p Weaviate t·∫°i: {weaviate_url} v·ªõi collection: {collection_name}")

    try:
        # K·∫øt n·ªëi t·ªõi Weaviate
        client = weaviate_client
        if not client:
            logger.error("üî∏Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Weaviate.")
            return None
        # T√™n collection c·∫ßn ki·ªÉm tra
        collection_name = config.WEAVIATE_COLLECTION_NAME

        # Ki·ªÉm tra xem collection c√≥ t·ªìn t·∫°i kh√¥ng
        collection_exists = client.collections.exists(collection_name)

        logger.info(f"Collection {collection_name} exists: {collection_exists}")

        if chunks is not None and not collection_exists:
            logger.info(f"üî∏T·∫°o Weaviate collection m·ªõi t·ª´ {len(chunks)} chunks...")

            # Ki·ªÉm tra m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n
            logger.info(f"üî∏Chunk ƒë·∫ßu ti√™n:\n{chunks[0].metadata}")
            logger.info(f"üî∏N·ªôi dung:\n{chunks[0].page_content[:500]}...")

            # L·ªçc metadata ƒë·ªÉ ƒë·∫£m b·∫£o t∆∞∆°ng th√≠ch v·ªõi Weaviate
            chunks = filter_and_serialize_complex_metadata(chunks)
            logger.info(f"üî∏Metadata chunk ƒë·∫ßu ti√™n sau khi l·ªçc/serialize:\n{chunks[0].metadata}")

             # KI·ªÇM TRA TYPE
            if chunks:
                logger.info(f"Type c·ªßa chunk ƒë·∫ßu ti√™n: {type(chunks[0])}")
                # Ki·ªÉm tra xem c√≥ ph·∫£i l√† langchain Document kh√¥ng
                from langchain_core.documents import Document as LangchainDocument
                is_langchain_doc = isinstance(chunks[0], LangchainDocument)
                logger.info(f"Chunk ƒë·∫ßu ti√™n c√≥ ph·∫£i l√† langchain_core.documents.Document kh√¥ng? {is_langchain_doc}")
                if not is_langchain_doc:
                    logger.error("!!! L·ªñI NGHI√äM TR·ªåNG: Chunks kh√¥ng ph·∫£i l√† instance c·ªßa langchain_core.documents.Document")
                    # In ra c√°c attribute c·ªßa object ƒë·ªÉ xem n√≥ l√† g√¨
                    try:
                        logger.error(f"Attributes c·ªßa chunk[0]: {dir(chunks[0])}")
                        if hasattr(chunks[0], "metadata"):
                             logger.error(f"Metadata c·ªßa chunk[0] (n·∫øu c√≥): {chunks[0].metadata}")
                        if hasattr(chunks[0], "page_content"):
                             logger.error(f"Page_content c·ªßa chunk[0] (n·∫øu c√≥): {chunks[0].page_content[:100]}")
                    except:
                        pass # B·ªè qua n·∫øu kh√¥ng th·ªÉ dir()
                    return None # D·ª´ng ·ªü ƒë√¢y n·∫øu type sai



            # T·∫°o vectorstore
            max_batch_size = 1000  # K√≠ch th∆∞·ªõc batch an to√†n
            total_chunks = len(chunks)
            logger.info("üî∏ƒêang nh√∫ng d·ªØ li·ªáu...")

            # T·∫°o collection m·ªõi
            vectorstore = WeaviateVectorStore.from_documents(
                documents=chunks[:1],  # Kh·ªüi t·∫°o v·ªõi 1 t√†i li·ªáu ƒë·ªÉ t·∫°o schema
                embedding=embeddings,
                client=client,
                index_name=collection_name,
                text_key="text",  # T√™n tr∆∞·ªùng vƒÉn b·∫£n trong t√†i li·ªáu

                # by_texts=False # N·∫øu d√πng ids th√¨ kh√¥ng c·∫ßn by_texts, nh∆∞ng ƒë·ªÉ r√µ r√†ng
            )

            # Th√™m t√†i li·ªáu theo batch
            for i in range(1, total_chunks, max_batch_size):
                end_idx = min(i + max_batch_size, total_chunks)
                current_batch = chunks[i:end_idx]
                logger.info(f"üî∏ƒêang x·ª≠ l√Ω batch {i//max_batch_size + 1}/{(total_chunks-1)//max_batch_size + 1}: t·ª´ {i} ƒë·∫øn {end_idx-1}")

                try:
                    vectorstore.add_documents(current_batch)
                    logger.info(f"üî∏ƒê√£ th√™m batch {i//max_batch_size + 1} th√†nh c√¥ng")
                except Exception as batch_error:
                    logger.error(f"üî∏L·ªói khi x·ª≠ l√Ω batch t·ª´ {i} ƒë·∫øn {end_idx-1}: {str(batch_error)}")
                    # Th·ª≠ v·ªõi batch nh·ªè h∆°n
                    smaller_batch_size = max_batch_size // 2
                    if smaller_batch_size >= 10:
                        logger.info(f"üî∏Th·ª≠ l·∫°i v·ªõi batch size nh·ªè h∆°n: {smaller_batch_size}")
                        for j in range(i, end_idx, smaller_batch_size):
                            end_j = min(j + smaller_batch_size, end_idx)
                            smaller_batch = chunks[j:end_j]
                            try:
                                vectorstore.add_documents(smaller_batch)
                                logger.info(f"üî∏ƒê√£ th√™m batch nh·ªè t·ª´ {j} ƒë·∫øn {end_j-1} th√†nh c√¥ng")
                            except Exception as small_batch_error:
                                logger.error(f"üî∏V·∫´n l·ªói v·ªõi batch nh·ªè h∆°n t·ª´ {j} ƒë·∫øn {end_j-1}: {str(small_batch_error)}")
                    else:
                        logger.error(f"üî∏Batch size ƒë√£ qu√° nh·ªè, kh√¥ng th·ªÉ gi·∫£m th√™m. B·ªè qua batch n√†y.")
            logger.info(f"üî∏T·∫°o Weaviate collection th√†nh c√¥ng: {collection_name}")

        elif collection_exists:
            logger.info(f"üî∏T·∫£i Weaviate collection ƒë√£ t·ªìn t·∫°i: {collection_name}")
            vectorstore = WeaviateVectorStore(
                client=client,
                index_name=collection_name,
                embedding=embeddings,
                text_key="text",
                attributes=[ # Li·ªát k√™ T·∫§T C·∫¢ c√°c metadata b·∫°n c·∫ßn ƒë·ªÉ retriever ho·∫°t ƒë·ªông
                    "nam_ban_hanh", "title", "source", "field", "loai_van_ban", "so_hieu",
                    "ten_van_ban", "ngay_ban_hanh_str", "co_quan_ban_hanh", "entity_type",
                    # C√°c tr∆∞·ªùng serialize th√†nh JSON c≈©ng c·∫ßn ƒë∆∞·ª£c li·ªát k√™ n·∫øu mu·ªën l·∫•y v·ªÅ
                    "cross_references", "penalties"
                ]
            )
            logger.info("üî∏T·∫£i Weaviate collection th√†nh c√¥ng.")

        else:
            logger.error(f"üî∏Collection '{collection_name}' kh√¥ng t·ªìn t·∫°i v√† kh√¥ng c√≥ d·ªØ li·ªáu chunks ƒë·ªÉ t·∫°o m·ªõi.")
            return None

        logger.info("üî∏Vectorstore s·∫µn s√†ng.")
        return vectorstore

    except Exception as e:
        if client:
            client.close()
            logger.info("üî∏ƒê√£ ƒë√≥ng k·∫øt n·ªëi t·ªõi Weaviate.")
        logger.error(f"üî∏L·ªói khi t·∫°o/t·∫£i Weaviate vector store: {e}")
        return None


def get_google_llm(google_api_key):
    logger.info("üî∏ƒêang kh·ªüi t·∫°o LLM t·ª´ Google Generative AI...")
    if not google_api_key:
        logger.error("üî∏Google API Key kh√¥ng ƒë∆∞·ª£c cung c·∫•p.")
        return None
    try:
        def create_chat_google():
            return ChatGoogleGenerativeAI(
                model="gemini-2.5-flash-preview-05-20",
                google_api_key=google_api_key,
                temperature=0.0, # ƒêi·ªÅu ch·ªânh nhi·ªát ƒë·ªô n·∫øu c·∫ßn, 0.1-0.3 th∆∞·ªùng t·ªët cho RAG
                safety_settings={                 },
            )

        llm = create_chat_google()

        logger.info("üî∏Kh·ªüi t·∫°o Google Generative AI LLM th√†nh c√¥ng.")
        return llm
    except Exception as e:
        logger.error(f"üî∏L·ªói khi kh·ªüi t·∫°o Google Generative AI LLM: {e}")
        return None


def create_qa_chain(
    llm: Any,
    retriever: Any, # Nh·∫≠n retriever n√¢ng cao ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o
    process_input_llm: Any = None
):
    """
    PHI√äN B·∫¢N CU·ªêI C√ôNG: T·∫°o ra m·ªôt RAG chain ho√†n ch·ªânh, t·ªëi ∆∞u h√≥a v·ªõi:
    1. Unified Pre-processing: M·ªôt l·ªánh g·ªçi LLM ƒë·ªÉ hi·ªÉu l·ªãch s·ª≠, "d·ªãch" thu·∫≠t ng·ªØ, v√† ph√¢n lo·∫°i.
    2. Multi-route: ƒê·ªãnh tuy·∫øn th√¥ng minh ƒë·∫øn c√°c nh√°nh x·ª≠ l√Ω chuy√™n bi·ªát.
    3. Advanced Retriever: S·ª≠ d·ª•ng retriever t√πy ch·ªânh cho nh√°nh ph√°p lu·∫≠t.
    """
    if not all([llm, retriever]):
        logger.error("üî∏ Thi·∫øu LLM ho·∫∑c Retriever ch√≠nh ƒë·ªÉ t·∫°o QA Chain.")
        return None

    try:
        logger.info("üî∏ B·∫Øt ƒë·∫ßu t·∫°o QA Chain T·ªëi ∆∞u (phi√™n b·∫£n cu·ªëi c√πng)...")

        # LLM cho b∆∞·ªõc ti·ªÅn x·ª≠ l√Ω (th∆∞·ªùng l√† model m·∫°nh nh·∫•t)
        preprocessing_llm = process_input_llm or llm

        # ----- PROMPTS (S·ª≠ d·ª•ng c√°c phi√™n b·∫£n ƒë√£ c·∫£i ti·∫øn) -----

        # 1. Prompt ti·ªÅn x·ª≠ l√Ω h·ª£p nh·∫•t
        # S·ª≠ d·ª•ng phi√™n b·∫£n V5 m·∫°nh m·∫Ω nh·∫•t ƒë·ªÉ "d·ªãch" thu·∫≠t ng·ªØ hi·ªáu qu·∫£
        unified_preprocessing_prompt = ChatPromptTemplate.from_template(
            prompt_templete.UNIFIED_PREPROCESSING_PROMPT
        )

        # 2. Prompt ƒë·ªÉ t·∫°o c√¢u tr·∫£ l·ªùi RAG t·ª´ context
        # S·ª≠ d·ª•ng phi√™n b·∫£n V4 ƒë·ªÉ "d·∫°y" LLM c√°ch ph√¢n t√≠ch v√† ∆∞u ti√™n th√¥ng tin
        qa_prompt = ChatPromptTemplate.from_template(
            prompt_templete.QA_PROMPT_TEMPLATE
        )

        # 3. C√°c prompt cho c√°c nh√°nh kh√°c
        persona_prompt = ChatPromptTemplate.from_messages([
            ("system", prompt_templete.GENERAL_PROMPT),
            ("human", "{input}")
        ])


        # ----- STEP 1: UNIFIED PREPROCESSING CHAIN -----
        # ƒê√¢y l√† b·ªô n√£o x·ª≠ l√Ω ƒë·∫ßu v√†o, thay th·∫ø cho 3 l·ªánh g·ªçi LLM c≈©
        unified_preprocessing_chain = (
            unified_preprocessing_prompt
            | preprocessing_llm
            | JsonOutputParser()
        ).with_config({"run_name": "UnifiedQuestionPreprocessor"})

        # ----- STEP 2: DEFINE BRANCHES (C√ÅC NH√ÅNH X·ª¨ L√ù) -----

        # --- Nh√°nh 1: LEGAL (RAG) ---
        # S·ª≠ d·ª•ng retriever n√¢ng cao ƒë√£ ƒë∆∞·ª£c truy·ªÅn v√†o
        legal_chain = (
            # `retriever` nh·∫≠n `rewritten_question` t·ª´ dict ƒë·∫ßu v√†o
            RunnablePassthrough.assign(context=itemgetter("rewritten_question") | retriever)
            # Chu·∫©n b·ªã input cho qa_prompt cu·ªëi c√πng
            .assign(input=itemgetter("rewritten_question"))
            | {
                "answer": qa_prompt | llm | StrOutputParser(),
                "context": itemgetter("context") # Gi·ªØ l·∫°i context ƒë·ªÉ c√≥ th·ªÉ hi·ªÉn th·ªã ngu·ªìn
            }
        ).with_config({"run_name": "AdvancedLegalRAGChain"})



        # --- Nh√°nh 3: GENERAL CHAT ---
        general_chat_chain = (
            {"input": itemgetter("rewritten_question")}
            | persona_prompt
            | llm
            | StrOutputParser()
            | (lambda answer: {"answer": answer, "context": []})
        ).with_config({"run_name": "GeneralChatChain"})

        # ----- STEP 3: ROUTER -----
        # ƒê·ªãnh nghƒ©a c√°c nh√°nh m√† router c√≥ th·ªÉ ch·ªçn
        branches = {
            "legal_rag": legal_chain,
            "general_chat": general_chat_chain,
            # Th√™m nh√°nh legal_term_explanation ·ªü ƒë√¢y n·∫øu b·∫°n tri·ªÉn khai n√≥
        }

        def route_branches(info: dict):
            """H√†m ƒë·ªãnh tuy·∫øn, ch·ªçn chain ph√π h·ª£p d·ª±a tr√™n k·∫øt qu·∫£ ph√¢n lo·∫°i."""
            classification = info.get("classification", "general_chat")
            logger.info(f"Routing to branch: '{classification}'")
            # Ch·ªçn chain, m·∫∑c ƒë·ªãnh l√† general_chat n·∫øu c√≥ l·ªói
            return branches.get(classification, general_chat_chain)

        # ----- STEP 4: FULL CHAIN -----
        # K·∫øt h·ª£p th√†nh m·ªôt chu·ªói x·ª≠ l√Ω duy nh·∫•t v√† li·ªÅn m·∫°ch
        # Lu·ªìng: Input -> Ti·ªÅn x·ª≠ l√Ω (Vi·∫øt l·∫°i + Ph√¢n lo·∫°i) -> Router -> Ch·∫°y nh√°nh ƒë∆∞·ª£c ch·ªçn
        full_chain = unified_preprocessing_chain | RunnableLambda(route_branches)

        logger.info("‚úÖ Successfully created Final Optimized QA Chain.")
        return full_chain

    except Exception as e:
        logger.error(f"‚ùå Error creating QA Chain: {e}", exc_info=True)
        return None